{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wtemp/585\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[np.argmax(y)])\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[np.argmax(y)] -= 1.0\n",
    "        return probs\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2\n",
    "\n",
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:#指的是左右方向的layer\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "\n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff_s\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=5):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        layers = []\n",
    "        prev_s = np.zeros(self.hidden_dim)\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            layer = RNNLayer()\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[np.argmax(x[t])] = 1\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
    "            prev_s = layer.s\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
    "\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        loss = 0.0\n",
    "        for i, layer in enumerate(layers):\n",
    "            loss += output.loss(layer.mulv, y[i])\n",
    "        return loss / float(len(y))\n",
    "\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.calculate_loss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers)\n",
    "        prev_s_t = np.zeros(self.hidden_dim)\n",
    "        diff_s = np.zeros(self.hidden_dim)# 只第一个用了一次初始化\n",
    "        for t in range(0,T):\n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[np.argmax(x[t])] = 1\n",
    "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "            prev_s_t = layers[t].s\n",
    "            dmulv = np.zeros(self.word_dim)\n",
    "            for i in range(t,0,-1):\n",
    "                input = np.zeros(self.word_dim)\n",
    "                input[np.argmax(x[i])] = 1\n",
    "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.W -= learning_rate * dW\n",
    "\n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_total_loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                #print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                # if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                #     learning_rate = learning_rate * 0.5\n",
    "                #     print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[4, 11, 11, 14]\n",
      "[(0, 3.2460093625435875), (1, 3.2336551455210962), (2, 3.2213084821186477), (3, 3.208960488141744), (4, 3.1966023382712483), (5, 3.1842252459928124), (6, 3.171820444019158), (7, 3.1593791651242475), (8, 3.146892623314663), (9, 3.134351995269455), (10, 3.121748401985532), (11, 3.109072890571339), (12, 3.0963164161373835), (13, 3.0834698237380582), (14, 3.0705238303255498), (15, 3.0574690066833465), (16, 3.0442957593143225), (17, 3.030994312266691), (18, 3.0175546888905243), (19, 3.0039666935283584), (20, 2.990219893155834), (21, 2.976303599002856), (22, 2.9622068482026505), (23, 2.947918385535959), (24, 2.9334266453608455), (25, 2.918719733845936), (26, 2.903785411657007), (27, 2.888611077284518), (28, 2.873183751243868), (29, 2.857490061431945), (30, 2.8415162299840167), (31, 2.8252480620456084), (32, 2.8086709369561023), (33, 2.791769802436015), (34, 2.774529172479883), (35, 2.756933129783273), (36, 2.738965333677245), (37, 2.7206090347084664), (38, 2.7018470971893414), (39, 2.682662031251112), (40, 2.6630360361640126), (41, 2.642951056941358), (42, 2.6223888565162676), (43, 2.6013311060656337), (44, 2.5797594963475348), (45, 2.5576558732028665), (46, 2.5350024006307503), (47, 2.511781755054007), (48, 2.4879773545096207), (49, 2.4635736264820087), (50, 2.43855631788232), (51, 2.4129128501874733), (52, 2.386632721894059), (53, 2.359707959104509), (54, 2.33213361312366), (55, 2.3039083012768176), (56, 2.2750347836499185), (57, 2.2455205640172546), (58, 2.2153784978493416), (59, 2.1846273840845836), (60, 2.153292510574936), (61, 2.121406116280901), (62, 2.089007727181848), (63, 2.056144318581293), (64, 2.022870255400114), (65, 1.9892469657005842), (66, 1.955342312527581), (67, 1.9212296461999603), (68, 1.8869865435305953), (69, 1.8526932708083086), (70, 1.8184310407419289), (71, 1.7842801652677744), (72, 1.750318230337832), (73, 1.71661842967996), (74, 1.6832481876573595), (75, 1.6502681753136192), (76, 1.6177317809504448), (77, 1.5856850435012615), (78, 1.554167002557841), (79, 1.5232103727763513), (80, 1.4928424203688593), (81, 1.463085909725615), (82, 1.4339599988177734), (83, 1.4054809889027249), (84, 1.3776628706526122), (85, 1.3505176478077427), (86, 1.3240554542766916), (87, 1.2982845066179665), (88, 1.2732109487643442), (89, 1.2488386496796036), (90, 1.2251690091315892), (91, 1.2022008147674774), (92, 1.1799301783882956), (93, 1.1583505636906604), (94, 1.1374529040534982), (95, 1.117225798603954), (96, 1.097655768338488), (97, 1.0787275513249366), (98, 1.0604244163095826), (99, 1.0427284765064186), (100, 1.0256209890364874), (101, 1.0090826296187232), (102, 0.9930937360938017), (103, 0.9776345178088385), (104, 0.9626852306156353), (105, 0.9482263191994813), (106, 0.9342385297192998), (107, 0.9207029964179323), (108, 0.9076013060882925), (109, 0.8949155441885241), (110, 0.8826283261018005), (111, 0.8707228166258647), (112, 0.8591827403223531), (113, 0.8479923849035262), (114, 0.837136599414041), (115, 0.8266007885941311), (116, 0.8163709044946328), (117, 0.8064334361538148), (118, 0.79677539793714), (119, 0.7873843169777774), (120, 0.7782482200311397), (121, 0.7693556199639546), (122, 0.7606955020310346), (123, 0.7522573100453306), (124, 0.7440309325142449), (125, 0.7360066887936196), (126, 0.7281753152971369), (127, 0.7205279517906825), (128, 0.7130561277966218), (129, 0.7057517491305899), (130, 0.6986070845923492), (131, 0.6916147528317714), (132, 0.6847677094107392), (133, 0.6780592340813238), (134, 0.671482918299971), (135, 0.66503265299645), (136, 0.6587026166150588), (137, 0.65248726344403), (138, 0.6463813122473059), (139, 0.6403797352109218), (140, 0.6344777472141919), (141, 0.6286707954338261), (142, 0.6229545492870213), (143, 0.6173248907175694), (144, 0.6117779048270904), (145, 0.6063098708516932), (146, 0.6009172534826935), (147, 0.5955966945284914), (148, 0.5903450049133518), (149, 0.5851591570076049), (150, 0.5800362772827521), (151, 0.5749736392840428), (152, 0.5699686569123357), (153, 0.5650188780064438), (154, 0.5601219782166525), (155, 0.5552757551597308), (156, 0.5504781228454728), (157, 0.5457271063646281), (158, 0.5410208368279725), (159, 0.5363575465462581), (160, 0.5317355644408064), (161, 0.5271533116746073), (162, 0.5226092974939339), (163, 0.5181021152706472), (164, 0.5136304387355934), (165, 0.5091930183937331), (166, 0.5047886781118947), (167, 0.5004163118703366), (168, 0.49607488066958466), (169, 0.4917634095843143), (170, 0.4874809849563564), (171, 0.4832267517192101), (172, 0.47899991084676197), (173, 0.4747997169192157), (174, 0.4706254757995437), (175, 0.46647654241408204), (176, 0.4623523186311724), (177, 0.4582522512320667), (178, 0.45417582996857503), (179, 0.45012258570223146), (180, 0.446092088620011), (181, 0.44208394652190286), (182, 0.4380978031758955), (183, 0.434133336736171), (184, 0.43019025822055823), (185, 0.426268310043512), (186, 0.4223672646011152), (187, 0.41848692290481726), (188, 0.41462711326082374), (189, 0.41078768999226345), (190, 0.40696853220143786), (191, 0.40316954256966314), (192, 0.39939064619237225), (193, 0.3956317894473397), (194, 0.3918929388940393), (195, 0.3881740802023217), (196, 0.3844752171087349), (197, 0.3807963703989762), (198, 0.3771375769150921), (199, 0.3734988885861882), (200, 0.3698803714815318), (201, 0.36628210488506635), (202, 0.3627041803904611), (203, 0.3591467010159463), (204, 0.35560978033827967), (205, 0.35209354164530304), (206, 0.34859811710663025), (207, 0.345123646962112), (208, 0.3416702787277988), (209, 0.3382381664192053), (210, 0.33482746979175065), (211, 0.3314383535983227), (212, 0.3280709868639686), (213, 0.3247255421777765), (214, 0.3214021950020565), (215, 0.31810112299898713), (216, 0.3148225053749155), (217, 0.3115665222425463), (218, 0.3083333540012774), (219, 0.3051231807359627), (220, 0.30193618163439934), (221, 0.2987725344238529), (222, 0.2956324148269305), (223, 0.292515996037127), (224, 0.2894234482143526), (225, 0.28635493800075285), (226, 0.28331062805711144), (227, 0.28029067662011775), (228, 0.27729523708074877), (229, 0.2743244575840032), (230, 0.2713784806501878), (231, 0.2684574428179315), (232, 0.26556147430906074), (233, 0.26269069871544265), (234, 0.25984523270785637), (235, 0.2570251857669147), (236, 0.25423065993601357), (237, 0.25146174959624934), (238, 0.24871854126319234), (239, 0.24600111340536174), (240, 0.24330953628420615), (241, 0.24064387181534255), (242, 0.23800417345076827), (243, 0.23539048608170776), (244, 0.23280284596172482), (245, 0.2302412806496773), (246, 0.2277058089720593), (247, 0.22519644100423664), (248, 0.2227131780700382), (249, 0.22025601275914358), (250, 0.21782492896166372), (251, 0.21541990191929075), (252, 0.2130408982923621), (253, 0.2106878762421631), (254, 0.20836078552776752), (255, 0.2060595676167037), (256, 0.20378415580871162), (257, 0.20153447537185246), (258, 0.1993104436902121), (259, 0.19711197042244843), (260, 0.1949389576704128), (261, 0.1927913001570914), (262, 0.1906688854131005), (263, 0.18857159397098291), (264, 0.18649929956656092), (265, 0.1844518693466023), (266, 0.1824291640820789), (267, 0.1804310383863032), (268, 0.1784573409372462), (269, 0.17650791470336286), (270, 0.1745825971722633), (271, 0.17268122058159385), (272, 0.17080361215151182), (273, 0.16894959431816584), (274, 0.16711898496760966), (275, 0.165311597669612), (276, 0.1635272419108448), (277, 0.161765723326961), (278, 0.16002684393309827), (279, 0.15831040235237573), (280, 0.15661619404197405), (281, 0.15494401151641646), (282, 0.1532936445677019), (283, 0.15166488048195662), (284, 0.15005750425230965), (285, 0.14847129878771198), (286, 0.14690604511745659), (287, 0.14536152259116925), (288, 0.1438375090740727), (289, 0.14233378113734835), (290, 0.14085011424343913), (291, 0.1393862829261633), (292, 0.1379420609655272), (293, 0.1365172215571446), (294, 0.13511153747619328), (295, 0.13372478123585188), (296, 0.13235672524018133), (297, 0.1310071419314319), (298, 0.1296758039317673), (299, 0.12836248417941806), (300, 0.12706695605928414), (301, 0.1257889935280243), (302, 0.12452837123367516), (303, 0.12328486462986073), (304, 0.1220582500846545), (305, 0.12084830498417544), (306, 0.119654807830998), (307, 0.11847753833746484), (308, 0.11731627751400761), (309, 0.11617080775256929), (310, 0.11504091290524554), (311, 0.11392637835825101), (312, 0.11282699110133114), (313, 0.11174253979273799), (314, 0.11067281481989222), (315, 0.1096176083558571), (316, 0.10857671441174879), (317, 0.10754992888521195), (318, 0.10653704960508674), (319, 0.1055378763723979), (320, 0.1045522109977915), (321, 0.10357985733554928), (322, 0.10262062131430616), (323, 0.10167431096459657), (324, 0.10074073644335627), (325, 0.0998197100555005), (326, 0.09891104627270167), (327, 0.09801456174948281), (328, 0.09713007533675035), (329, 0.09625740809287318), (330, 0.09539638329242664), (331, 0.09454682643270948), (332, 0.09370856523814163), (333, 0.09288142966264518), (334, 0.0920652518901156), (335, 0.0912598663330773), (336, 0.09046510962962366), (337, 0.08968082063873194), (338, 0.08890684043404647), (339, 0.08814301229621457), (340, 0.08738918170386586), (341, 0.08664519632330761), (342, 0.08591090599702421), (343, 0.08518616273105246), (344, 0.08447082068130492), (345, 0.08376473613891591), (346, 0.08306776751467441), (347, 0.08237977532261084), (348, 0.08170062216279883), (349, 0.08103017270343446), (350, 0.08036829366224656), (351, 0.0797148537872953), (352, 0.07906972383721354), (353, 0.07843277656093528), (354, 0.07780388667696451), (355, 0.07718293085222734), (356, 0.07656978768055252), (357, 0.07596433766081781), (358, 0.07536646317480793), (359, 0.07477604846481486), (360, 0.07419297961102034), (361, 0.07361714450869225), (362, 0.07304843284522607), (363, 0.07248673607706473), (364, 0.07193194740652027), (365, 0.07138396175852986), (366, 0.07084267575736496), (367, 0.07030798770332565), (368, 0.0697797975494335), (369, 0.06925800687815223), (370, 0.06874251887815044), (371, 0.06823323832112735), (372, 0.06773007153871895), (373, 0.06723292639949995), (374, 0.06674171228609771), (375, 0.06625634007243003), (376, 0.06577672210108455), (377, 0.06530277216084579), (378, 0.06483440546438705), (379, 0.06437153862613272), (380, 0.06391408964030384), (381, 0.063461977859155), (382, 0.06301512397140895), (383, 0.06257344998089936), (384, 0.06213687918542573), (385, 0.061705336155827434), (386, 0.06127874671528272), (387, 0.06085703791883893), (388, 0.06044013803317336), (389, 0.060027976516595666), (390, 0.05962048399928939), (391, 0.059217592263799156), (392, 0.05881923422576184), (393, 0.05842534391489125), (394, 0.058035856456208464), (395, 0.05765070805152808), (396, 0.05726983596119473), (397, 0.05689317848607472), (398, 0.056520674949801314), (399, 0.05615226568127392), (400, 0.05578789199741198), (401, 0.055427496186163494), (402, 0.05507102148976277), (403, 0.05471841208824632), (404, 0.054369613083214786), (405, 0.054024570481848694), (406, 0.053683231181170514), (407, 0.053345542952555725), (408, 0.053011454426488275), (409, 0.05268091507756039), (410, 0.05235387520971424), (411, 0.052030285941723244), (412, 0.05171009919291154), (413, 0.051393267669107935), (414, 0.05107974484883418), (415, 0.050769484969722896), (416, 0.05046244301516545), (417, 0.050158574701182505), (418, 0.04985783646352146), (419, 0.04956018544497158), (420, 0.04926557948289842), (421, 0.0489739770969926), (422, 0.048685337477231326), (423, 0.048399620472049584), (424, 0.04811678657671689), (425, 0.04783679692192111), (426, 0.04755961326254895), (427, 0.04728519796666817), (428, 0.04701351400470401), (429, 0.046744524938808034), (430, 0.04647819491241655), (431, 0.04621448863999691), (432, 0.04595337139697558), (433, 0.045694809009850526), (434, 0.04543876784647898), (435, 0.04518521480654173), (436, 0.04493411731218133), (437, 0.044685443298808254), (438, 0.044439161206075085), (439, 0.044195239969015004), (440, 0.043953649009342725), (441, 0.043714358226912914), (442, 0.043477337991335685), (443, 0.043242559133746285), (444, 0.04300999293872459), (445, 0.04277961113636321), (446, 0.04255138589448356), (447, 0.04232528981099174), (448, 0.042101295906377394), (449, 0.041879377616351056), (450, 0.041659508784615946), (451, 0.0414416636557756), (452, 0.0412258168683683), (453, 0.04101194344803557), (454, 0.04080001880081287), (455, 0.04059001870654657), (456, 0.04038191931243103), (457, 0.040175697126668165), (458, 0.03997132901224111), (459, 0.03976879218080492), (460, 0.03956806418669094), (461, 0.03936912292102046), (462, 0.0391719466059286), (463, 0.03897651378889533), (464, 0.03878280333718098), (465, 0.038590794432363656), (466, 0.03840046656498045), (467, 0.038211799529265046), (468, 0.03802477341798482), (469, 0.03783936861737105), (470, 0.03765556580214596), (471, 0.03747334593063839), (472, 0.03729269023999188), (473, 0.03711358024145981), (474, 0.03693599771578912), (475, 0.03675992470868655), (476, 0.03658534352637074), (477, 0.036412236731204445), (478, 0.036240587137409276), (479, 0.03607037780685658), (480, 0.03590159204493717), (481, 0.03573421339650702), (482, 0.03556822564190665), (483, 0.035403612793053464), (484, 0.03524035908960689), (485, 0.035078448995200726), (486, 0.034917867193748245), (487, 0.03475859858581122), (488, 0.0346006282850375), (489, 0.03444394161466257), (490, 0.0342885241040744), (491, 0.03413436148544181), (492, 0.033981439690402696), (493, 0.03382974484681411), (494, 0.03367926327555931), (495, 0.03352998148741401), (496, 0.033381886179968144), (497, 0.03323496423460462), (498, 0.03308920271353138), (499, 0.03294458885686674)]\n"
     ]
    }
   ],
   "source": [
    "element_dict = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10\n",
    "                ,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,\n",
    "                'u':20,'v':21,'w':22,'x':23,'y':24,'z':25}\n",
    "\n",
    "np.random.seed(10)\n",
    "rnn = Model(26, 100)\n",
    "\n",
    "input_word = 'hello'\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "word = []\n",
    "for i in range(len(input_word)):\n",
    "  element_vector = [0]*26\n",
    "  element_vector[element_dict[input_word[i]]]=1\n",
    "  word.append(element_vector)\n",
    "\n",
    "X_train.append(word[:4])\n",
    "y_train.append(word[1:])\n",
    "\n",
    "losses = rnn.train(X_train, y_train, learning_rate=0.005, nepoch=500, evaluate_loss_after=1)\n",
    "print(\"213\", word)\n",
    "print(rnn.predict(word[:4]))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3.2460093625435875), (1, 3.2336551455210962), (2, 3.2213084821186477), (3, 3.208960488141744), (4, 3.1966023382712483), (5, 3.1842252459928124), (6, 3.171820444019158), (7, 3.1593791651242475), (8, 3.146892623314663), (9, 3.134351995269455), (10, 3.121748401985532), (11, 3.109072890571339), (12, 3.0963164161373835), (13, 3.0834698237380582), (14, 3.0705238303255498), (15, 3.0574690066833465), (16, 3.0442957593143225), (17, 3.030994312266691), (18, 3.0175546888905243), (19, 3.0039666935283584), (20, 2.990219893155834), (21, 2.976303599002856), (22, 2.9622068482026505), (23, 2.947918385535959), (24, 2.9334266453608455), (25, 2.918719733845936), (26, 2.903785411657007), (27, 2.888611077284518), (28, 2.873183751243868), (29, 2.857490061431945), (30, 2.8415162299840167), (31, 2.8252480620456084), (32, 2.8086709369561023), (33, 2.791769802436015), (34, 2.774529172479883), (35, 2.756933129783273), (36, 2.738965333677245), (37, 2.7206090347084664), (38, 2.7018470971893414), (39, 2.682662031251112), (40, 2.6630360361640126), (41, 2.642951056941358), (42, 2.6223888565162676), (43, 2.6013311060656337), (44, 2.5797594963475348), (45, 2.5576558732028665), (46, 2.5350024006307503), (47, 2.511781755054007), (48, 2.4879773545096207), (49, 2.4635736264820087), (50, 2.43855631788232), (51, 2.4129128501874733), (52, 2.386632721894059), (53, 2.359707959104509), (54, 2.33213361312366), (55, 2.3039083012768176), (56, 2.2750347836499185), (57, 2.2455205640172546), (58, 2.2153784978493416), (59, 2.1846273840845836), (60, 2.153292510574936), (61, 2.121406116280901), (62, 2.089007727181848), (63, 2.056144318581293), (64, 2.022870255400114), (65, 1.9892469657005842), (66, 1.955342312527581), (67, 1.9212296461999603), (68, 1.8869865435305953), (69, 1.8526932708083086), (70, 1.8184310407419289), (71, 1.7842801652677744), (72, 1.750318230337832), (73, 1.71661842967996), (74, 1.6832481876573595), (75, 1.6502681753136192), (76, 1.6177317809504448), (77, 1.5856850435012615), (78, 1.554167002557841), (79, 1.5232103727763513), (80, 1.4928424203688593), (81, 1.463085909725615), (82, 1.4339599988177734), (83, 1.4054809889027249), (84, 1.3776628706526122), (85, 1.3505176478077427), (86, 1.3240554542766916), (87, 1.2982845066179665), (88, 1.2732109487643442), (89, 1.2488386496796036), (90, 1.2251690091315892), (91, 1.2022008147674774), (92, 1.1799301783882956), (93, 1.1583505636906604), (94, 1.1374529040534982), (95, 1.117225798603954), (96, 1.097655768338488), (97, 1.0787275513249366), (98, 1.0604244163095826), (99, 1.0427284765064186), (100, 1.0256209890364874), (101, 1.0090826296187232), (102, 0.9930937360938017), (103, 0.9776345178088385), (104, 0.9626852306156353), (105, 0.9482263191994813), (106, 0.9342385297192998), (107, 0.9207029964179323), (108, 0.9076013060882925), (109, 0.8949155441885241), (110, 0.8826283261018005), (111, 0.8707228166258647), (112, 0.8591827403223531), (113, 0.8479923849035262), (114, 0.837136599414041), (115, 0.8266007885941311), (116, 0.8163709044946328), (117, 0.8064334361538148), (118, 0.79677539793714), (119, 0.7873843169777774), (120, 0.7782482200311397), (121, 0.7693556199639546), (122, 0.7606955020310346), (123, 0.7522573100453306), (124, 0.7440309325142449), (125, 0.7360066887936196), (126, 0.7281753152971369), (127, 0.7205279517906825), (128, 0.7130561277966218), (129, 0.7057517491305899), (130, 0.6986070845923492), (131, 0.6916147528317714), (132, 0.6847677094107392), (133, 0.6780592340813238), (134, 0.671482918299971), (135, 0.66503265299645), (136, 0.6587026166150588), (137, 0.65248726344403), (138, 0.6463813122473059), (139, 0.6403797352109218), (140, 0.6344777472141919), (141, 0.6286707954338261), (142, 0.6229545492870213), (143, 0.6173248907175694), (144, 0.6117779048270904), (145, 0.6063098708516932), (146, 0.6009172534826935), (147, 0.5955966945284914), (148, 0.5903450049133518), (149, 0.5851591570076049), (150, 0.5800362772827521), (151, 0.5749736392840428), (152, 0.5699686569123357), (153, 0.5650188780064438), (154, 0.5601219782166525), (155, 0.5552757551597308), (156, 0.5504781228454728), (157, 0.5457271063646281), (158, 0.5410208368279725), (159, 0.5363575465462581), (160, 0.5317355644408064), (161, 0.5271533116746073), (162, 0.5226092974939339), (163, 0.5181021152706472), (164, 0.5136304387355934), (165, 0.5091930183937331), (166, 0.5047886781118947), (167, 0.5004163118703366), (168, 0.49607488066958466), (169, 0.4917634095843143), (170, 0.4874809849563564), (171, 0.4832267517192101), (172, 0.47899991084676197), (173, 0.4747997169192157), (174, 0.4706254757995437), (175, 0.46647654241408204), (176, 0.4623523186311724), (177, 0.4582522512320667), (178, 0.45417582996857503), (179, 0.45012258570223146), (180, 0.446092088620011), (181, 0.44208394652190286), (182, 0.4380978031758955), (183, 0.434133336736171), (184, 0.43019025822055823), (185, 0.426268310043512), (186, 0.4223672646011152), (187, 0.41848692290481726), (188, 0.41462711326082374), (189, 0.41078768999226345), (190, 0.40696853220143786), (191, 0.40316954256966314), (192, 0.39939064619237225), (193, 0.3956317894473397), (194, 0.3918929388940393), (195, 0.3881740802023217), (196, 0.3844752171087349), (197, 0.3807963703989762), (198, 0.3771375769150921), (199, 0.3734988885861882), (200, 0.3698803714815318), (201, 0.36628210488506635), (202, 0.3627041803904611), (203, 0.3591467010159463), (204, 0.35560978033827967), (205, 0.35209354164530304), (206, 0.34859811710663025), (207, 0.345123646962112), (208, 0.3416702787277988), (209, 0.3382381664192053), (210, 0.33482746979175065), (211, 0.3314383535983227), (212, 0.3280709868639686), (213, 0.3247255421777765), (214, 0.3214021950020565), (215, 0.31810112299898713), (216, 0.3148225053749155), (217, 0.3115665222425463), (218, 0.3083333540012774), (219, 0.3051231807359627), (220, 0.30193618163439934), (221, 0.2987725344238529), (222, 0.2956324148269305), (223, 0.292515996037127), (224, 0.2894234482143526), (225, 0.28635493800075285), (226, 0.28331062805711144), (227, 0.28029067662011775), (228, 0.27729523708074877), (229, 0.2743244575840032), (230, 0.2713784806501878), (231, 0.2684574428179315), (232, 0.26556147430906074), (233, 0.26269069871544265), (234, 0.25984523270785637), (235, 0.2570251857669147), (236, 0.25423065993601357), (237, 0.25146174959624934), (238, 0.24871854126319234), (239, 0.24600111340536174), (240, 0.24330953628420615), (241, 0.24064387181534255), (242, 0.23800417345076827), (243, 0.23539048608170776), (244, 0.23280284596172482), (245, 0.2302412806496773), (246, 0.2277058089720593), (247, 0.22519644100423664), (248, 0.2227131780700382), (249, 0.22025601275914358), (250, 0.21782492896166372), (251, 0.21541990191929075), (252, 0.2130408982923621), (253, 0.2106878762421631), (254, 0.20836078552776752), (255, 0.2060595676167037), (256, 0.20378415580871162), (257, 0.20153447537185246), (258, 0.1993104436902121), (259, 0.19711197042244843), (260, 0.1949389576704128), (261, 0.1927913001570914), (262, 0.1906688854131005), (263, 0.18857159397098291), (264, 0.18649929956656092), (265, 0.1844518693466023), (266, 0.1824291640820789), (267, 0.1804310383863032), (268, 0.1784573409372462), (269, 0.17650791470336286), (270, 0.1745825971722633), (271, 0.17268122058159385), (272, 0.17080361215151182), (273, 0.16894959431816584), (274, 0.16711898496760966), (275, 0.165311597669612), (276, 0.1635272419108448), (277, 0.161765723326961), (278, 0.16002684393309827), (279, 0.15831040235237573), (280, 0.15661619404197405), (281, 0.15494401151641646), (282, 0.1532936445677019), (283, 0.15166488048195662), (284, 0.15005750425230965), (285, 0.14847129878771198), (286, 0.14690604511745659), (287, 0.14536152259116925), (288, 0.1438375090740727), (289, 0.14233378113734835), (290, 0.14085011424343913), (291, 0.1393862829261633), (292, 0.1379420609655272), (293, 0.1365172215571446), (294, 0.13511153747619328), (295, 0.13372478123585188), (296, 0.13235672524018133), (297, 0.1310071419314319), (298, 0.1296758039317673), (299, 0.12836248417941806), (300, 0.12706695605928414), (301, 0.1257889935280243), (302, 0.12452837123367516), (303, 0.12328486462986073), (304, 0.1220582500846545), (305, 0.12084830498417544), (306, 0.119654807830998), (307, 0.11847753833746484), (308, 0.11731627751400761), (309, 0.11617080775256929), (310, 0.11504091290524554), (311, 0.11392637835825101), (312, 0.11282699110133114), (313, 0.11174253979273799), (314, 0.11067281481989222), (315, 0.1096176083558571), (316, 0.10857671441174879), (317, 0.10754992888521195), (318, 0.10653704960508674), (319, 0.1055378763723979), (320, 0.1045522109977915), (321, 0.10357985733554928), (322, 0.10262062131430616), (323, 0.10167431096459657), (324, 0.10074073644335627), (325, 0.0998197100555005), (326, 0.09891104627270167), (327, 0.09801456174948281), (328, 0.09713007533675035), (329, 0.09625740809287318), (330, 0.09539638329242664), (331, 0.09454682643270948), (332, 0.09370856523814163), (333, 0.09288142966264518), (334, 0.0920652518901156), (335, 0.0912598663330773), (336, 0.09046510962962366), (337, 0.08968082063873194), (338, 0.08890684043404647), (339, 0.08814301229621457), (340, 0.08738918170386586), (341, 0.08664519632330761), (342, 0.08591090599702421), (343, 0.08518616273105246), (344, 0.08447082068130492), (345, 0.08376473613891591), (346, 0.08306776751467441), (347, 0.08237977532261084), (348, 0.08170062216279883), (349, 0.08103017270343446), (350, 0.08036829366224656), (351, 0.0797148537872953), (352, 0.07906972383721354), (353, 0.07843277656093528), (354, 0.07780388667696451), (355, 0.07718293085222734), (356, 0.07656978768055252), (357, 0.07596433766081781), (358, 0.07536646317480793), (359, 0.07477604846481486), (360, 0.07419297961102034), (361, 0.07361714450869225), (362, 0.07304843284522607), (363, 0.07248673607706473), (364, 0.07193194740652027), (365, 0.07138396175852986), (366, 0.07084267575736496), (367, 0.07030798770332565), (368, 0.0697797975494335), (369, 0.06925800687815223), (370, 0.06874251887815044), (371, 0.06823323832112735), (372, 0.06773007153871895), (373, 0.06723292639949995), (374, 0.06674171228609771), (375, 0.06625634007243003), (376, 0.06577672210108455), (377, 0.06530277216084579), (378, 0.06483440546438705), (379, 0.06437153862613272), (380, 0.06391408964030384), (381, 0.063461977859155), (382, 0.06301512397140895), (383, 0.06257344998089936), (384, 0.06213687918542573), (385, 0.061705336155827434), (386, 0.06127874671528272), (387, 0.06085703791883893), (388, 0.06044013803317336), (389, 0.060027976516595666), (390, 0.05962048399928939), (391, 0.059217592263799156), (392, 0.05881923422576184), (393, 0.05842534391489125), (394, 0.058035856456208464), (395, 0.05765070805152808), (396, 0.05726983596119473), (397, 0.05689317848607472), (398, 0.056520674949801314), (399, 0.05615226568127392), (400, 0.05578789199741198), (401, 0.055427496186163494), (402, 0.05507102148976277), (403, 0.05471841208824632), (404, 0.054369613083214786), (405, 0.054024570481848694), (406, 0.053683231181170514), (407, 0.053345542952555725), (408, 0.053011454426488275), (409, 0.05268091507756039), (410, 0.05235387520971424), (411, 0.052030285941723244), (412, 0.05171009919291154), (413, 0.051393267669107935), (414, 0.05107974484883418), (415, 0.050769484969722896), (416, 0.05046244301516545), (417, 0.050158574701182505), (418, 0.04985783646352146), (419, 0.04956018544497158), (420, 0.04926557948289842), (421, 0.0489739770969926), (422, 0.048685337477231326), (423, 0.048399620472049584), (424, 0.04811678657671689), (425, 0.04783679692192111), (426, 0.04755961326254895), (427, 0.04728519796666817), (428, 0.04701351400470401), (429, 0.046744524938808034), (430, 0.04647819491241655), (431, 0.04621448863999691), (432, 0.04595337139697558), (433, 0.045694809009850526), (434, 0.04543876784647898), (435, 0.04518521480654173), (436, 0.04493411731218133), (437, 0.044685443298808254), (438, 0.044439161206075085), (439, 0.044195239969015004), (440, 0.043953649009342725), (441, 0.043714358226912914), (442, 0.043477337991335685), (443, 0.043242559133746285), (444, 0.04300999293872459), (445, 0.04277961113636321), (446, 0.04255138589448356), (447, 0.04232528981099174), (448, 0.042101295906377394), (449, 0.041879377616351056), (450, 0.041659508784615946), (451, 0.0414416636557756), (452, 0.0412258168683683), (453, 0.04101194344803557), (454, 0.04080001880081287), (455, 0.04059001870654657), (456, 0.04038191931243103), (457, 0.040175697126668165), (458, 0.03997132901224111), (459, 0.03976879218080492), (460, 0.03956806418669094), (461, 0.03936912292102046), (462, 0.0391719466059286), (463, 0.03897651378889533), (464, 0.03878280333718098), (465, 0.038590794432363656), (466, 0.03840046656498045), (467, 0.038211799529265046), (468, 0.03802477341798482), (469, 0.03783936861737105), (470, 0.03765556580214596), (471, 0.03747334593063839), (472, 0.03729269023999188), (473, 0.03711358024145981), (474, 0.03693599771578912), (475, 0.03675992470868655), (476, 0.03658534352637074), (477, 0.036412236731204445), (478, 0.036240587137409276), (479, 0.03607037780685658), (480, 0.03590159204493717), (481, 0.03573421339650702), (482, 0.03556822564190665), (483, 0.035403612793053464), (484, 0.03524035908960689), (485, 0.035078448995200726), (486, 0.034917867193748245), (487, 0.03475859858581122), (488, 0.0346006282850375), (489, 0.03444394161466257), (490, 0.0342885241040744), (491, 0.03413436148544181), (492, 0.033981439690402696), (493, 0.03382974484681411), (494, 0.03367926327555931), (495, 0.03352998148741401), (496, 0.033381886179968144), (497, 0.03323496423460462), (498, 0.03308920271353138), (499, 0.03294458885686674)]\n"
     ]
    }
   ],
   "source": [
    "RNN_Letter_v1_time\n",
    "for item in loss:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
